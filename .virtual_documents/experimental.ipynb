import torch
from torch import nn, optim
import torch.nn.functional as F
from torch.nn.modules.utils import _pair
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.gridspec as gridspec
import matplotlib.patheffects as pe


%load_ext autoreload
%autoreload 2





from baseline import Baseline


from genhebb import HebbianLayer, GenHebb


from sklearn.decomposition import PCA


import umap





def load_model(plasticity='none', wta='none', unsup_epochs=50, sup_epochs=50, model_name=None):
    directory = 'saved_models/done-training'
    learning_rule = plasticity if wta == 'none' else f'{wta}_WTA_{plasticity}'
    if not model_name:
        model_name = f'genhebb-{learning_rule}-{unsup_epochs}_unsup_epochs-50_sup_epochs-0.001_unsup_lr-0.001_sup_lr-64_batch'
    path = f'{directory}/{model_name}.pt'
    if model_name.split('-')[0] == 'genhebb':
        model = GenHebb(28*28, 2000, 10, 1, plasticity, wta)
    else:
        model = Baseline(28*28, 2000, 10)
    model.load_state_dict(torch.load(path))
    return model


def load_data(sample_size=1000):
    trainset = FastMNIST('./data', train=True, download=True)
    images, labels = next(iter(DataLoader(trainset, batch_size=sample_size, shuffle=True)))
    x = images.reshape(-1, 28*28)
    return x, labels


def get_projection(x):
    pca = PCA(n_components=2)
    x_proj = pca.fit_transform(x)
    x_expl_var = pca.explained_variance_ratio_
    x_expl_var = [round(var, 3) for var in x_expl_var]
    return x_proj, x_expl_var


def get_umap(x):
    reducer = umap.UMAP()
    x_umap = reducer.fit_transform(x)
    return x_umap


# load original data and get projection
x, labels = load_data(sample_size=1000)
x_proj, x_expl_var = get_projection(x)
x_umap = get_umap(x)


# get projections/umaps for all hebbian-learned embeddings
proj_embeddings = {}
umap_embeddings = {}
expl_vars = {}

for plasticity, wta in [('ojas_rule', 'hard')]:
    learning_rule = plasticity if wta == 'none' else f'{wta}_WTA_{plasticity}'
    for unsup_epochs, sup_epochs in [(1, 50), (10, 50)]:  # NOTE: add (50, 50), (100, 100)

        # get projected embedding
        model = load_model(plasticity, wta, unsup_epochs, sup_epochs)
        Wx = model.hebb(x).detach()
        # Wx_proj, Wx_expl_var = get_projection(Wx)
        Wx_umap = get_umap(Wx)

        # add to dicts
        key = f'{learning_rule}-{unsup_epochs}'
        # proj_embeddings[key] = Wx_proj
        umap_embeddings[key] = Wx_umap
        # expl_vars[key] = Wx_expl_var
        print(f'Done for {key}')


# get baseline embeddings
baseline = load_model(model_name='baseline-50_epochs-0.001_lr-64_batch')
Wx = baseline.input(x).detach()
# baseline_proj, baseline_expl_var = get_projection(Wx)
baseline_umap = get_umap(Wx)

# get random_W embeddings
random_W = load_model(plasticity='random_W', model_name='genhebb-random_W-1_unsup_epochs-50_sup_epochs-0.001_unsup_lr-0.001_sup_lr-64_batch')
Wx = random_W.hebb(x).detach()
# random_W_proj, random_W_expl_var = get_projection(Wx)
random_W_umap = get_umap(Wx)


fig, axes = plt.subplots(2, 3, figsize=(12, 8))

# plot for original and baselines
axes[0, 0].scatter(x_umap[:, 0], x_umap[:, 1], c=labels, cmap='Paired', s=5)
axes[0, 1].scatter(baseline_umap[:, 0], baseline_umap[:, 1], c=labels, cmap='Paired', s=5)
axes[0, 2].scatter(random_W_umap[:, 0], random_W_umap[:, 1], c=labels, cmap='Paired', s=5)
axes[0, 0].text(0.02, 0.95, 'original', transform=axes[0, 0].transAxes)
axes[0, 1].text(0.02, 0.95, 'baseline (BP)', transform=axes[0, 1].transAxes)
axes[0, 2].text(0.02, 0.95, 'random_W', transform=axes[0, 2].transAxes)
# axes[0, 3].axis('off')

# plot for Hebbian learning rules
for i, plasticity, wta in [(1, 'ojas_rule', 'hard')]:
    for j, unsup_epochs in [(0, 1), (1, 10), (2, 50)]:
        ax = axes[i, j]
        umap_embedding = umap_embeddings[f'{learning_rule}-{unsup_epochs}']
        ax.scatter(umap_embedding[:,0], umap_embedding[:,1], c=labels, cmap='Paired', s=5)
        ax.text(0.02, 0.9, f'{learning_rule}\nunsup_epochs={unsup_epochs}', transform=ax.transAxes)

# formatting
fig.suptitle('Embeddings from Learning Rules', fontsize=18)
plt.tight_layout()
# fig.savefig('figures/embeddings.png', dpi=300)





def get_neuron_similarities(model, num_samples=1000):
    # get and normalize weight matrix
    try:
        W = model.unsup_layer.W
    except:
        W = model.input.weight
    W_normed = F.normalize(W)
    
    # get pairwise cosine similarities
    similarities = torch.matmul(W_normed, W_normed.T)
    similarities = similarities[~torch.eye(similarities.size(0), dtype=bool)].flatten().detach()
    
    # sample similarities
    indices = torch.randperm(similarities.size(0))[:num_samples]
    similarities_sample = similarities[indices].numpy()

    return similarities_sample


fig, axes = plt.subplots(4, 4, figsize=(16, 16), sharex=True, sharey=True)

# plot for baselines
axes[0, 0].hist(get_neuron_similarities(baseline), bins=20, range=(-1,1), weights=np.ones(num_samples)/num_samples, edgecolor='black')
axes[0, 1].hist(get_neuron_similarities(random_W), bins=20, range=(-1,1), weights=np.ones(num_samples)/num_samples, edgecolor='black')
axes[0, 0].text(0.02, 0.95, 'baseline (BP)', transform=axes[0, 0].transAxes)
axes[0, 1].text(0.02, 0.95, 'random_W', transform=axes[0, 1].transAxes)
axes[0, 2].axis('off')
axes[0, 3].axis('off')

# plot for Hebbian learning rules
for i, learning_rule in [(1, 'hebbs_rule'), (2, 'ojas_rule'), (3, 'hard_WTA_ojas_rule')]:
    for j, unsup_epochs in [(0, 1), (1, 5), (2, 10), (3, 50)]:
        ax = axes[i, j]
        model = load_model(learning_rule=learning_rule, unsup_epochs=unsup_epochs)
        ax.hist(get_neuron_similarities(model), bins=20, range=(-1,1), weights=np.ones(num_samples)/num_samples, edgecolor='black')
        ax.text(0.02, 0.9, f'{learning_rule}\nunsup_epochs={unsup_epochs}', transform=ax.transAxes)

# formatting
fig.suptitle('Distribution of Neuronal Cosine Similarities for Learning Rules', fontsize=18)
for j in range(4):
    axes[3, j].set_xlabel('Neuronal Cosine Similarities')
for i in range(4):
    axes[i, 0].set_ylabel('Fraction')
plt.tight_layout()
fig.savefig('figures/neuronal-similarities.png', dpi=300)





n_neurons=100
weights = []
learning_rule = 'hard_WTA_ojas_rule'
for epoch in range(1, 51):
    model = load_model(learning_rule=learning_rule, mid_training=True, epoch=epoch)
    weights.append(model.unsup_layer.W.flatten().detach().numpy()[:784*n_neurons])

plt.figure(figsize=(20, 5), dpi=300)
plt.pcolormesh(weights, cmap='Oranges')
for n in range(1, n_neurons):
    plt.axvline(n*784, c='cyan')
plt.colorbar()
plt.title("Ojas's Rule")
plt.tight_layout()
plt.show()





import random


baseline = load_model(model_name='baseline-50_epochs-0.001_lr-64_batch')


ojas_rule = load_model('ojas_rule', 'hard', 50)


trainset = FastMNIST('./data', train=True, download=True)


samples = {}
for image, label in trainset:
    if label.item() not in samples.keys():
        samples[label.item()] = image


samples = dict(sorted(samples.items()))


topk[0]


fig, axes = plt.subplots(2, 10, figsize=(20, 4))
_, topk = torch.topk(ojas_rule.hebb[0](samples[0].view(-1, 28*28)), 10)
for i in range(10):

    # get most activated hebbian neuron
    y_max = torch.argmax(ojas_rule.hebb[0](samples[i].view(-1, 28*28)))

    # plot baseline and hebbian learned weights
    axes[0,i].imshow(baseline.input.weight[i].view(28, 28).detach())
    axes[1,i].imshow(ojas_rule.hebb[0].W[topk.flatten()[i]].view(28, 28).detach())
    axes[1,i].text(0.02, 0.92, topk.flatten()[i].item(), transform=axes[1,i].transAxes, c='white')

    # formatting
    axes[0,i].set_xticks([])
    axes[0,i].set_yticks([])
    axes[1,i].set_xticks([])
    axes[1,i].set_yticks([])
plt.tight_layout()


def plot_weights(learning_rule, learning_params='none', n_hebbian_layers=1, unsup_epochs=1, sup_epochs=50, unsup_lr=0.001, save=False):

    # parse model name
    directory = 'models/saved/done-training/'
    # model_name = (
    #     f'genhebb-{learning_rule}-{learning_params}'
    #     f'-{n_hebbian_layers}_hebbian_layers-100_hidden_dim'
    #     f'-{unsup_epochs}_unsup_epochs-{sup_epochs}_sup_epochs'
    #     f'-{unsup_lr}_unsup_lr-0.001_sup_lr'
    #      '-64_unsup_batch-64_sup_batch'
    # )
    model_name = 'temp'
    path = directory + model_name + '.pt'

    # load model
    model = GenHebb(28*28, 100, 10, learning_rule, n_hebbian_layers)
    model.load_state_dict(torch.load(path))

    # plot weights for each layer
    fig = plt.figure(figsize=(n_hebbian_layers*10, 10))
    axes = []
    outer_grid = gridspec.GridSpec(1, n_hebbian_layers, width_ratios=n_hebbian_layers*[1], wspace=0.02)
    for layer in range(n_hebbian_layers):
        inner_grid = gridspec.GridSpecFromSubplotSpec(10, 10, subplot_spec=outer_grid[layer], wspace=0.05, hspace=0.05)
        for i, grid in enumerate(inner_grid):
            ax = plt.Subplot(fig, grid)
            dim = 28 if layer == 0 else 10
            im = ax.imshow(model.hebb[layer].W[i].view(dim, dim).detach(), cmap='viridis')
            ax.set_xticks([])
            ax.set_yticks([])
            fig.add_subplot(ax)
            if layer == n_hebbian_layers - 1:
                axes.append(ax)

    # highlight most important weights for each class
    for d in range(10):
        i_max = torch.argmax(model.classifier.weight[d]).item()
        ax = axes[i_max]
        for spine in ax.spines.values():
            spine.set_edgecolor('orange')
            spine.set_linewidth(3)
        ax.text(0.1, 0.1, d, transform=ax.transAxes, fontsize=15, color='orange',
               path_effects=[pe.withStroke(linewidth=1, foreground='black')])
    cbar_ax = fig.add_axes([0.92, 0.3, 0.01, 0.4])
    fig.colorbar(im, cax=cbar_ax)
    plt.show()
    if save:
        fig.savefig(f'figures/weights-{learning_rule}-{learning_params}-{n_hebbian_layers}_hebbian_layers-{unsup_lr}_unsup_lr.png', dpi=300)


from genhebb import GenHebb


plot_weights('STDP', n_hebbian_layers=1)


plot_weights('soft_WTA', n_hebbian_layers=1)


# plot_weights('STDP', n_hebbian_layers=2, unsup_epochs=1, sup_epochs=50, unsup_lr=3e-5, save=False)


# plot_weights('soft_WTA', 'beta=1.0', n_hebbian_layers=2, unsup_epochs=10, unsup_lr=0.01, save=False)


from baseline import Baseline


# load model
path = 'saved_models/done-training/baseline-1_hidden_layers-0.2_dropout_p-50_epochs-0.001_lr-64_batch.pt'
model = Baseline(28*28, 100, 10, 1, 0.2)
model.load_state_dict(torch.load(path))

# plot weights
fig, axes = plt.subplots(10, 10, figsize=(10, 10))
for i, ax in enumerate(axes.flat):
    im = ax.imshow(model.hidden[0].weight[i].view(28, 28).detach(), cmap='viridis')
    ax.set_xticks([])
    ax.set_yticks([])
fig.suptitle(f'learning_rule=baseline (BP)', fontsize=14, y=0.96)
plt.tight_layout()
fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)
cbar_ax = fig.add_axes([0.92, 0.3, 0.02, 0.4])
fig.colorbar(im, ax=axes, cax=cbar_ax)
# fig.savefig('figures/weights-baseline.png', dpi=300)





input_dim = 784
hidden_dim = 2000


W1 = torch.randn(hidden_dim, input_dim)
W2 = torch.randn(hidden_dim, hidden_dim)


W1 = F.normalize(W1)
W2 = F.normalize(W2)


x = torch.randn(64, 784)
y1 = torch.matmul(x, W1.T)
y2 = torch.matmul(y1, W2.T)


dW1 = y1.unsqueeze(-1) * (x.unsqueeze(-2) - y1.unsqueeze(-1) * W1.unsqueeze(0))
dW1 = torch.mean(dW1, 0)


impW2 = torch.norm(W2, dim=0).unsqueeze(-1).expand_as(dW1)


torch.std(impW2[:,0])





model = Baseline(28*28, 100, 10, n_hidden_layers=2, dropout_p=0.2)
model.load_state_dict(torch.load('models/saved/done-training/temp.pt'))


model.hidden.requires_grad = False
model.hidden.eval()


optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()


trainloader, testloader = utils.load_data({'supervised': {'epochs': 50, 'lr': 0.001, 'batch_size': 64}}, 'supervised', 'cpu')


epochs = 10
for epoch in range(epochs):
    model.classifier.train()
    running_loss = 0.0
    correct = 0
    total = 0
    for inputs, labels in trainloader:

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # compute training statistics
        running_loss += loss.item()
        if epoch == 0 or epoch % 10 == 9 or epoch == epochs - 1:
            total += labels.size(0)
            _, predicted = torch.max(outputs.data, 1)
            correct += (predicted == labels).sum().item()

    # evaluation
    if epoch == 0 or epoch % 10 == 9 or epoch == epochs - 1:
        msg = (
            f'epoch [{epoch+1}/{epochs}]\n' +
            f'train loss: {running_loss / len(trainloader):.3f} \t train accuracy: {100 * correct / total:.1f} %'
        )
        print(msg)

        # on the test set
        model.eval()
        running_loss = 0.
        correct = 0
        total = 0
        # since we're not training, we don't need to calculate the gradients for our outputs
        with torch.no_grad():
            for data in testloader:
                inputs, labels = data
                inputs = inputs.to(device)
                labels = labels.to(device)
                # calculate outputs by running inputs through the network
                outputs = model(inputs)
                # the class with the highest energy is what we choose as prediction
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                loss = criterion(outputs, labels)
                running_loss += loss.item()
        msg = f'test loss: {running_loss / len(testloader):.3f} \t test accuracy: {100 * correct / total:.1f} % \n'
        print(msg)



