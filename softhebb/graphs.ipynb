{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10ebad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import os.path as op\n",
    "import time\n",
    "\n",
    "import foolbox\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import ticker\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import umap\n",
    "\n",
    "from dataset import make_data_loaders\n",
    "from model import load_layers\n",
    "from utils import load_config_dataset\n",
    "from utils import SEARCH\n",
    "import nb_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c104c9bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If you want to store the figures, change the path below\n",
    "STORE_GRAPH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e009fb0a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if 'science' in plt.style.available:\n",
    "    plt.style.reload_library()\n",
    "    plt.style.use(['science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b341ef4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blues = cm.get_cmap('Blues')\n",
    "greens = cm.get_cmap('Greens')\n",
    "reds = cm.get_cmap('Oranges')\n",
    "device ='cuda:0'\n",
    "font_size=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "    'CIFAR10':{\n",
    "        0 :'airplane',\n",
    "        1:'automobile',\n",
    "        2:'bird',\n",
    "        3:'cat',\n",
    "        4:'deer',\n",
    "        5:'dog',\n",
    "        6:'frog',\n",
    "        7:'horse',\n",
    "        8:'ship',\n",
    "        9:'truck'\n",
    "    },\n",
    "    'STL10':{\n",
    "        0 :'airplane',\n",
    "        1:'bird',\n",
    "        2:'car',\n",
    "        3:'cat',\n",
    "        4:'deer',\n",
    "        5:'dog',\n",
    "        6:'horse',\n",
    "        7:'monkey',\n",
    "        8:'ship',\n",
    "        9:'truck'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9840d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fb8be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "green = cm.get_cmap('Greens')\n",
    "x= ['1 layer', '2 layers', '3 layers']\n",
    "# Our results with 4 seeds\n",
    "softhebb_results_mean = [71.11,77.7,80.31]\n",
    "softhebb_results_std = [0.063442888,0.383234589,0.148421936]\n",
    "# Try to load results\n",
    "for n_layers in [2,3,4]:\n",
    "    try:\n",
    "        mean, std = nb_utils.get_mean_std(os.path.join(SEARCH, f'CIFAR10_SoftHebb{n_layers}'))\n",
    "        softhebb_results_mean[n_layers-2] = mean\n",
    "        softhebb_results_std[n_layers-2] = std\n",
    "    except:\n",
    "        print(f'No results found for {n_layers-1} layer. Using past results.')\n",
    "        \n",
    "y = [\n",
    "    [63.92,63.81,58.28],\n",
    "    [64.69,65.92,64.43],\n",
    "    [58.56,64.2,64.55],\n",
    "    softhebb_results_mean,\n",
    "]\n",
    "error=[[0.,0,0],\n",
    "       [0.29,0.14,0.21],\n",
    "       [0.3,0.4,0.4],\n",
    "       softhebb_results_std\n",
    "]\n",
    "font_size=25\n",
    "label = ['Amato et al., 2019','Lagani et al., 2021','Miconi, 2021', 'SoftHebb (ours)']\n",
    "colors = [green(i) for i in np.linspace(0.4,1,3)][::-1] + [ plt.cm.Blues(1.0)]\n",
    "plt.rcParams.update({'font.size': 40,\n",
    "                    'lines.linewidth':4,\n",
    "                    'lines.linestyle':'-',\n",
    "                    'lines.markersize':1})\n",
    "fig =  plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)#, aspect=0.08)\n",
    "for i in range(len(label))[::-1]:\n",
    "    plt.errorbar(x, y[i], error[i], color=colors[i], label=label[i])\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=font_size)\n",
    "#plt.xlabel(\"#layer\")\n",
    "plt.title(\"CIFAR10 depth comparison\", fontsize=font_size)\n",
    "plt.ylim((55,82))\n",
    "plt.xticks([0, 1, 2],x, fontsize=font_size)\n",
    "plt.minorticks_off()\n",
    "#ax.set_xticks(ax.get_xticks()[1:])\n",
    "#ax.set_xticks([])\n",
    "plt.yticks(np.linspace(56, 80,4), fontsize=font_size)\n",
    "\n",
    "plt.legend(fontsize=font_size, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "try:\n",
    "    fig.savefig(op.join(STORE_GRAPH, 'depth_comparison.png'))\n",
    "    fig.savefig(op.join(STORE_GRAPH, 'depth_comparison.svg'), format='svg')\n",
    "except:\n",
    "    print('---Save not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2330da5",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d6cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'SoftHebb_STL'\n",
    "name_model = 'STL10_SoftHebb5'\n",
    "# Figure 2\n",
    "n_neurons_per_layer = 72\n",
    "# Figures B.8, B.9, B.10, B.11 will all be generated when uncommenting next line\n",
    "# n_neurons_per_layer = 250\n",
    "\n",
    "model_checkpoint = nb_utils.find_checkpoint(os.path.join(SEARCH, name_model))\n",
    "assert model_checkpoint is not None, \"Checkpoint of trained model not found\"\n",
    "print(model_checkpoint)\n",
    "model = nb_utils.resume_model(name_model, model_path_override=os.path.join(model_checkpoint, 'checkpoint.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da346cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order weight for a better visualization\n",
    "weights = []\n",
    "w_indices = []\n",
    "from torch.nn import Linear\n",
    "for i in range(len(model.blocks)):\n",
    "    if isinstance(model.blocks[i].layer, Linear):\n",
    "        continue\n",
    "    weight = model.blocks[i].layer.weight.detach().cpu()\n",
    "    out_channel, in_channel, k1, k2 = weight.shape\n",
    "    weight_reshape = weight.reshape((out_channel, in_channel, (k1 * k2)))\n",
    "    order = weight_reshape.std(2).mean(1)\n",
    "    sorted, indices = torch.sort(order)\n",
    "    w_indices.append(indices)\n",
    "    weight = weight[indices]\n",
    "    weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_indices = [np.random.choice(iw.shape[0], size=min(n_neurons_per_layer, iw.shape[0]), replace=False) for iw in weights]\n",
    "NL = len(weights)\n",
    "receptive_fields = [5, 12, 26, 54]\n",
    "samples_per_objective = 1\n",
    "batch_size = 1\n",
    "exp_name = 'PGD_L2_randomStart'\n",
    "random_start = True\n",
    "load_samples = False\n",
    "overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b225667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "neurons_per_layer = [ids[:n_neurons_per_layer] for ids in w_indices]\n",
    "\n",
    "# ============== Based on std of filters ========================\n",
    "og_w_indices = w_indices\n",
    "new_w_indices = []\n",
    "from torch.nn import Linear\n",
    "for i in range(len(model.blocks)):\n",
    "    if isinstance(model.blocks[i].layer, Linear):\n",
    "        continue\n",
    "    weight = model.blocks[i].layer.weight.detach().cpu()\n",
    "    out_channel, in_channel, k1, k2 = weight.shape\n",
    "    weight_reshape = weight.reshape((out_channel, in_channel, (k1 * k2)))\n",
    "    order = weight_reshape.std(2).mean(1)\n",
    "    order = order[og_w_indices[i]]\n",
    "    _, indices = torch.sort(order)\n",
    "    new_w_indices.append(og_w_indices[i][indices]) \n",
    "    \n",
    "\n",
    "neurons_per_layer = [ids[:n_neurons_per_layer] for ids in new_w_indices]\n",
    "epsilons = np.array([255]) / 255\n",
    "figs, imgs, successs, accs = [], [], [], []\n",
    "\n",
    "\n",
    "for l in range(NL):\n",
    "    print(f\"  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ Layer {l} \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ \")\n",
    "    h_w = receptive_fields[l]\n",
    "    model = nb_utils.resume_model(name_model, model_path_override=os.path.join(model_checkpoint, 'checkpoint.pth.tar'))\n",
    "    fig = plt.figure(figsize=(np.sqrt(len(neurons_per_layer[l])), np.sqrt(len(neurons_per_layer[l]))))\n",
    "    for i_n, n in enumerate(neurons_per_layer[l]): \n",
    "        print(f\" ===================== Neuron {n} ({i_n}-th) ===================== \")\n",
    "        #ax = plt.subplot(np.ceil(np.sqrt(N[topl-1])),np.ceil(np.sqrt(N[topl-1])),i_n+1)\n",
    "        ax = plt.subplot(np.ceil(np.sqrt(len(neurons_per_layer[l]))),np.ceil(np.sqrt(len(neurons_per_layer[l]))),i_n+1)\n",
    "        #model = resume_model(name_model)\n",
    "        i_model = model.blocks[:l+1]\n",
    "        for prev_l in range(l):\n",
    "            i_model[prev_l].pool.padding = 0\n",
    "            i_model[prev_l].layer.F_padding, i_model[prev_l].layer.F_pad = 0, False\n",
    "        i_model[-1].layer.F_padding, i_model[-1].layer.F_pad = 0, False\n",
    "        i_model[-1].activation = None # remove last activation to max negative values too\n",
    "        i_model[-1].pool = None # remove pooling in last layer\n",
    "        i_model.add_module(str(len(i_model)), nb_utils.ChooseNeuronFlat(n))\n",
    "        i_model.eval()\n",
    "        model.to(device)\n",
    "        for s in range(samples_per_objective):\n",
    "            print(f\" ===================== Sample {s} ===================== \")\n",
    "            filename = f\"images/receptive_fields/FigRFs_{exp_name}/L_{l}_N_{n}_S_{s}.npy\"\n",
    "            \n",
    "            if not os.path.isdir(os.path.dirname(filename)):\n",
    "                os.makedirs(os.path.dirname(filename))\n",
    "            if load_samples or (os.path.isfile(filename) and not overwrite):\n",
    "                print('Loading existing image')\n",
    "                with open(filename, 'rb') as f:\n",
    "                    img_to_plot = np.load(f)\n",
    "                    # add sample dim (redundant as will be indexed) and batch dim\n",
    "                    img = torch.tensor(img_to_plot.transpose(2, 0, 1)[np.newaxis, np.newaxis, :, :, :]).to(device)\n",
    "            else:\n",
    "                data = torch.zeros(batch_size,3,h_w,h_w, device=torch.device(device))\n",
    "                # Might have to edit /username/anaconda3/envs/softhebb/lib/python3.8/site-packages/foolbox/attacks/base.py\n",
    "                criterion = nb_utils.SingleMax(max_val = 1e10, eps = 1e-05)\n",
    "                img, _, success = nb_utils.attack_model(i_model, data.clone(), None, epsilons, step_size=0.001, iterations=500,\n",
    "                                               random_start=random_start, nb_start=1, criterion=criterion, device=device)\n",
    "                img_to_plot = img[s, 0].transpose(-3,-2).transpose(-2,-1).cpu().numpy()\n",
    "                with open(filename, 'wb') as f:\n",
    "                    np.save(f, img_to_plot)\n",
    "        ax.imshow(img_to_plot)\n",
    "        # In case you want titles\n",
    "        #ax.set_title(f'N {n}, Act {i_model(img[-1]).item():.1f}', size=4, y=1.0)\n",
    "        ax.set_axis_off()\n",
    "        fig.tight_layout()\n",
    "        k1, k2, in_c = img_to_plot.shape\n",
    "        assert in_c == 3\n",
    "        print(f'STD {img_to_plot.reshape((k1 * k2), in_c).std(0).mean()}')\n",
    "        \n",
    "    figs.append(fig)\n",
    "    imgs.append(img)\n",
    "    successs.append(success)\n",
    "    acc = (1 - success.cpu().numpy().mean(axis=-1)) * 100\n",
    "    accs.append(acc)\n",
    "    # print(\"for model {}, took Time : {}\".format(name, \n",
    "    #             time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - t))))\n",
    "    for eps, a in zip(epsilons, acc):\n",
    "        print(\"Epsilon: {} tTest Accuracy = {}\".format(eps, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec815d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to store resulting figures\n",
    "# for numl in range(NL):\n",
    "#     file = \"FigRFs\"+str(numl+1)+f\"_{exp_name}_{n_neurons_per_layer}\"\n",
    "# #     with open(file+\".npy\", 'wb') as f:\n",
    "# #         np.save(f, w_indeces[numl])\n",
    "#     figs[numl].savefig(file+\".pdf\", format='pdf')\n",
    "#     figs[numl].savefig(file+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624ea85",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a200b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'STL10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb80151",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_step = 1\n",
    "width = x_step/1.5\n",
    "models = ['SoftHebb', 'Hard WTA',  'Random', 'Backprop e2e']\n",
    "# models = ['SoftHebb',]\n",
    "label2model = {'SoftHebb': 'SoftHebb', 'Hard WTA': 'HardWTA',  'Random': 'Random', 'Backprop e2e': 'BP'}\n",
    "if dataset == 'CIFAR10':\n",
    "    nb_layer = 3\n",
    "    ratio=0.4\n",
    "    x =models *nb_layer\n",
    "    \n",
    "    y_ticks =np.linspace(57, 84, 4)\n",
    "    y_lim =(56,89)\n",
    "    acc_means = [71.10, 62.69,  63.99, 72.42,\n",
    "                 77.70,60.08,  68.64, 82.49, \n",
    "                 80.31, 60.29,  64.62,  83.97]\n",
    "    acc_stds = [0.06, 0.47,  0.21, 0.22, 0.24,\n",
    "                0.38,0.49, 0.24, 0.35, 0.12, \n",
    "                0.14,0.47,0.25, 0.15,0.07]        \n",
    "    \n",
    "    \n",
    "    x_pos = [x_step*i+x_step*((i > 3)+(i > 7)) for i in range(len(x))]\n",
    "    \n",
    "elif dataset == 'CIFAR100':\n",
    "    nb_layer = 3\n",
    "    ratio=0.4\n",
    "    x = models*nb_layer\n",
    "    \n",
    "    y_ticks =np.linspace(20, 56, 4)\n",
    "    y_lim =(19,60)\n",
    "    acc_means = [43.00, 34.34, 34.59, 44.76,\n",
    "                 52.31,33.49,  38.76, 52.11, \n",
    "                 56.0,21.6, 38.25,  57.19]\n",
    "    acc_stds = [0.16,0.49,  0.21, 0.88, 0.14,\n",
    "                0.14,0.51, 0.24, 0.4, 0.21, \n",
    "                0.14,1.,0.25, 0.87, 0.07]\n",
    "    \n",
    "    \n",
    "    x_pos = [x_step*i+x_step*((i > 3)+(i > 7)) for i in range(len(x))]\n",
    "\n",
    "elif dataset == 'STL10':\n",
    "    nb_layer = 4\n",
    "    ratio=0.5\n",
    "    x = models*nb_layer\n",
    "    \n",
    "    y_ticks =np.linspace(52, 79, 4)\n",
    "    y_lim =(51,80)\n",
    "    acc_means = [65.4,57.92, 62.55, 67.5,\n",
    "                 70.11,58.4,  65.69, 69.64, \n",
    "                 73.1,54.31,  68.12,  72.93,\n",
    "                 76.23, 54.84,  68.19,  74.51]\n",
    "    acc_stds = [0.18,0.41,  0.64, 0.1,\n",
    "                0.20,0.15,  0.30, 0.21, \n",
    "                0.23,0.23,0.28,0.12,\n",
    "                 0.19,0.56,0.34,0.36]\n",
    "    \n",
    "    x_pos = [x_step*i+x_step*((i > 3)+(i > 7)+(i > 11)+(i > 15)) for i in range(len(x))]\n",
    "\n",
    "elif dataset == 'ImageNette':\n",
    "    nb_layer = 5\n",
    "    x = models*nb_layer\n",
    "    ratio=0.6\n",
    "    y_ticks =np.linspace(53, 86, 4)\n",
    "    y_lim =(51,88)\n",
    "    acc_means = [61.03,56.77,  61, 61.91,\n",
    "                 71.29,60.79, 67.31, 72.32, \n",
    "                 75.69,60.66, 68.12,  76.48,\n",
    "                 78.76,59.60, 74.2,  81.5,\n",
    "                80.98,61.2, 74.1,  85.3]\n",
    "    acc_stds = [0.32,0.41,  0.64, 0.21,\n",
    "                0.7,0.15,  0.30, 0.21, \n",
    "                0.12,0.23,0.28,0.12,\n",
    "                 0.23,0.56,0.34,0.36,\n",
    "               0.43,0.56,0.34,0.45]\n",
    "    \n",
    "    x_pos = [x_step*i+x_step*((i > 3)+(i > 7)+(i > 11)+(i > 15)) for i in range(len(x))]\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "# try to load results instead of previous hardcoded results\n",
    "for i_layer in range(1, nb_layer+1):\n",
    "    for i_model, model_label in enumerate(models):\n",
    "        try:\n",
    "            mean, std = nb_utils.get_mean_std(os.path.join(SEARCH, f'{dataset}_{label2model[model_label]}{i_layer+1}'))\n",
    "            acc_means[(i_layer-1)*len(models)+i_model] = mean\n",
    "            acc_stds[(i_layer-1)*len(models)+i_model] = std    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Could not load {model_label} with {i_layer+1} layers. Using past results.')\n",
    "#             if model_label == 'SoftHebb' and i_layer+1 == 2:\n",
    "#                 import pdb\n",
    "#                 pdb.set_trace()\n",
    "                \n",
    "x_tick_pos = x_pos\n",
    "\n",
    "rgb0 ='C4'\n",
    "rgb1 ='C0'\n",
    "rgb2 ='C1'\n",
    "rgb3 ='C2'\n",
    "rgb4 ='C3'\n",
    "rgb5 ='C5'\n",
    "colors = [blues(1.0), greens(1.0), greens(0.6), greens(0.4)]*nb_layer\n",
    "\n",
    "font_size = 25\n",
    "\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1, aspect=ratio)\n",
    "for i in range(len(x)):\n",
    "    plt.bar(x_pos[i], acc_means[i], yerr=acc_stds[i], color=colors[i], width=width, label=x[i] if i<len(x)/nb_layer else None)\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=font_size)\n",
    "# plt.xlabel(\"Model\")\n",
    "#plt.title(\"Depth scaling\", fontsize=font_size)\n",
    "plt.ylim(y_lim)\n",
    "#plt.xticks(x_tick_pos, x, fontsize=14)\n",
    "ax.set_xticks([])\n",
    "plt.yticks(y_ticks, fontsize=font_size)\n",
    "\n",
    "\n",
    "if dataset=='CIFAR10':\n",
    "    plt.legend(fontsize=font_size)\n",
    "\n",
    "\n",
    "minorticks_loc = [x_step*1.5, 6.49*x_step, 11.49*x_step, 16.49*x_step, 21.49*x_step]\n",
    "ax.xaxis.set_minor_locator(ticker.FixedLocator(minorticks_loc))\n",
    "# ax.set_xticklabels(['\\n1 Layer', '\\n2 Layers'], minor=True)\n",
    "ax.set_xticklabels(['1 layer', '2 layers', '3 layers', '4 layers', '5 layers'], minor=True, fontsize=font_size)\n",
    "plt.tick_params(axis='x', which='minor', length=2, pad=5)\n",
    "#plt.minorticks_off()\n",
    "\n",
    "ax.yaxis.set_tick_params(which='minor', bottom=False)\n",
    "# ax.tick_params(axis=u'both', which=u'both')\n",
    "for i, v in enumerate(acc_means):\n",
    "    plt.text(x_pos[i]-0.5, v + 0.5, '{:.1f}'.format(v), color='black', fontsize=20)\n",
    "# ax.xaxis.set_minor_locator([0.5, 3])\n",
    "# ax.xaxis.set_minor_formatter(['\\nHard WTA', '\\nSoftHebb'])\n",
    "# ax.tick_params(axis='x', which='minor', length=0)\n",
    "plt.show()\n",
    "try:\n",
    "    fig.savefig(op.join(STORE_GRAPH, 'Depth_'+dataset+'.png'))\n",
    "    fig.savefig(op.join(STORE_GRAPH, 'Depth_'+dataset+'.svg'), format='svg')\n",
    "except:\n",
    "    print('---Save not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3a4b20",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57695e54",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = ['1 layer', '2 layers', '3 layers']\n",
    "\n",
    "\n",
    "acc_means = [[71.09497168,71.69210782,66.43911384],[71.09497168,74.85187749,73.86927136], [71.09497168,77.69155189,80.30996171]]\n",
    "acc_stds = [[0.063442888,0.23742104,0.343656806],[0.063442888,0.305399329,0.328176782],[0.063442888,0.383234589,0.148421936]]\n",
    "# try to load results instead of previous hardcoded results\n",
    "for i_layer in range(1, len(x)+1):\n",
    "    for i_model, model_label in enumerate(['wf1', 'wf2', 'wf4']):\n",
    "        model_label = '' if (model_label == 'wf4' or i_layer == 1) else f'_{model_label}'\n",
    "        try:\n",
    "            mean, std = nb_utils.get_mean_std(os.path.join(SEARCH, f'CIFAR10_SoftHebb{i_layer+1}{model_label}'))\n",
    "            acc_means[i_model][i_layer-1] = mean\n",
    "            acc_stds[i_model][i_layer-1] = std    \n",
    "        except:\n",
    "            print(f'Could not load {model_label} with {i_layer+1} layers. Using past results.')\n",
    "\n",
    "    \n",
    "x_step = 1\n",
    "width = x_step/1.5\n",
    "x_pos = [x_step*i+x_step/2*((i > 2)+(i > 5)) for i in range(9)]\n",
    "x_tick_pos = x_pos\n",
    "\n",
    "\n",
    "rgb0 ='C4'\n",
    "rgb1 ='C0'\n",
    "rgb2 ='C1'\n",
    "rgb3 ='C2'\n",
    "rgb4 ='C3'\n",
    "rgb5 ='C5'\n",
    "\n",
    "font_size = 25\n",
    "\n",
    "colors = [blues(i) for i in np.linspace(0.4,1,3)]*3\n",
    "\n",
    "plt.rcParams.update({'font.size': 18,\n",
    "                    'lines.linewidth':4,\n",
    "                    'lines.linestyle':'-',\n",
    "                    'lines.markersize':1})\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1, 1, 1, aspect=0.2)\n",
    "for i in range(len(x)):\n",
    "    plt.errorbar(x, acc_means[i], yerr=acc_stds[i], color=colors[i], label=['1x', '2x', '4x'][i] if i<3 else None)\n",
    "    \n",
    "# ax.bar(x,acc_means[2], color= green(1.0), width=0.2, alpha=1)\n",
    "plt.ylabel(\"Accuracy (\\%)\",fontsize=font_size)\n",
    "plt.title(\"Width scaling\",fontsize=font_size)\n",
    "plt.ylim((65,82))\n",
    "plt.xticks( x, fontsize=font_size)\n",
    "#ax.set_xticks([])\n",
    "plt.yticks(np.linspace(65, 81,5), fontsize=font_size)\n",
    "\n",
    "plt.legend(fontsize=font_size)\n",
    "plt.minorticks_off()\n",
    "\n",
    "plt.show()\n",
    "try:\n",
    "    fig.savefig(op.join(STORE_GRAPH, 'width_scaling.png'))\n",
    "    fig.savefig(op.join(STORE_GRAPH, 'width_scaling.svg'), format='svg')\n",
    "except:\n",
    "    print('---Save not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b4019",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe78e4",
   "metadata": {},
   "source": [
    "### A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be4fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.A\n",
    "# name_model, layer = 'STL10_SoftHebb5', 1\n",
    "# 5.B and B.6.A\n",
    "name_model, layer = 'STL10_SoftHebb5', 4\n",
    "# B.6.B\n",
    "# name_model, layer = 'STL10_Random5', 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532018b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gets one run. If you want a specific run, assing that path to model_checkpoint\n",
    "model_checkpoint = nb_utils.find_checkpoint(os.path.join(SEARCH, name_model))\n",
    "assert model_checkpoint is not None, \"Checkpoint of trained model not found\"\n",
    "print(model_checkpoint)\n",
    "model = nb_utils.resume_model(name_model, model_path_override=os.path.join(model_checkpoint, 'checkpoint.pth.tar'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sup = 'STL10_1'\n",
    "dataset_sup_config = load_config_dataset(dataset_sup, validation=False)\n",
    "dataset_unsup = 'STL10_100aug'\n",
    "dataset_unsup_config = load_config_dataset(dataset_unsup, validation=False)\n",
    "label_dict = labels_dict[dataset_sup.split('_')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ddbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_AFFINITY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3df6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = make_data_loaders(dataset_sup_config, 1000, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'UMAP'\n",
    "avg_pixels=False\n",
    "t=time.time()\n",
    "inputs, reps, targets = nb_utils.get_representations(model, test_loader, device, layer=layer, n_inputs_max=100000)\n",
    "reps, targets = nb_utils.rep_2d(inputs, reps, targets, avg_pixels=avg_pixels, method=method)\n",
    "print('Took', time.time()-t, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if layer == 0:\n",
    "    title = '%s of %s dataset'%(method, dataset_sup.split('_')[0])\n",
    "else:\n",
    "    title = '%s for %s dataset of layer %s'%(method, dataset_sup.split('_')[0], layer)\n",
    "w=40\n",
    "nb_utils.plot_2d(reps, targets, label_dict, plot_labels=layer==0, title=title, marker_size=50, no_border=True)#, xlim=[-w, w], ylim=[-w, w])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d1dfc",
   "metadata": {},
   "source": [
    "### C and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.A and B.7.A (for B.7.A run the blocks again with the configuration below for 5.B)\n",
    "name_model, layer = 'STL10_SoftHebb5', 1\n",
    "# 5.B and B.7.A (for B.7.A run the blocks again with the configuration above for 5.A)\n",
    "# name_model, layer = 'STL10_SoftHebb5', 4\n",
    "# B.7.B (run once each for layer=1 and 4)\n",
    "# name_model, layer = 'STL10_Random5', 1\n",
    "# name_model, layer = 'STL10_Random5', 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gets one run. If you want a specific run, assing that path to model_checkpoint\n",
    "model_checkpoint = nb_utils.find_checkpoint(os.path.join(SEARCH, name_model))\n",
    "assert model_checkpoint is not None, \"Checkpoint of trained model not found\"\n",
    "print(model_checkpoint)\n",
    "model = nb_utils.resume_model(name_model, model_path_override=os.path.join(model_checkpoint, 'checkpoint.pth.tar'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sup = 'STL10_1'\n",
    "dataset_sup_config = load_config_dataset(dataset_sup, validation=False)\n",
    "train_loader, test_loader = make_data_loaders(dataset_sup_config, 1000, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "inputs, reps, targets = nb_utils.get_representations(model, test_loader, device, layer=layer, n_inputs_max=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = nb_utils.max_activation(reps, n_neurons=5, n_max=10)\n",
    "patches = nb_utils.imgs_patches(model, layer, indexes)\n",
    "nb_utils.plot_patches(inputs, patches,'random_'+str(layer), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b8ae0",
   "metadata": {},
   "source": [
    "## Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d6d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy for SoftHebb on CIFAR10:', f\"{nb_utils.get_mean_std(os.path.join(SEARCH, f'CIFAR10_SoftHebb4'))[0]:.2f}\")\n",
    "print('Test accuracy for SoftHebb on CIFAR10:', f\"{nb_utils.get_mean_std(os.path.join(SEARCH, f'CIFAR10_BP4'))[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912dac4",
   "metadata": {},
   "source": [
    "## Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy for SoftHebb on STL10:', f\"{nb_utils.get_mean_std(os.path.join(SEARCH, f'STL10_SoftHebb5'))[0]:.2f}\")\n",
    "print('Test accuracy for Hard WTA on STL10:', f\"{nb_utils.get_mean_std(os.path.join(SEARCH, f'STL10_HardWTA5'))[0]:.2f}\")\n",
    "print('Test accuracy for Random on STL10:', f\"{nb_utils.get_mean_std(os.path.join(SEARCH, f'STL10_Random5'))[0]:.2f}\")\n",
    "print('Test accuracy for SoftHebb on ImageNet:', f\"{nb_utils.get_mean_std(os.path.join(SEARCH, f'ImageNet_SoftHebb5'))[0]:.2f}\")\n",
    "print('Test accuracy for Random on ImageNet:', f\"{nb_utils.get_mean_std(os.path.join(SEARCH, f'ImageNet_Random5'))[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a902f3",
   "metadata": {},
   "source": [
    "## B.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a08183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For B.1\n",
    "exp = 'SoftHebb2_regimes_multilayer'\n",
    "# For B.2\n",
    "# exp = 'SoftHebb4_regimes_multilayer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826268a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = nb_utils.get_data(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91007f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nb_utils.extract_data(datas, features = ['b0/layer/softness', 'b0/layer/t_invert', 'R1', 'test_acc', 'dataset_unsup/seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78549ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['t']<800]\n",
    "data = data.loc[data['t']>1/8000]\n",
    "data = [data.loc[data['b0/layer/softness'] == 'soft'], data.loc[data['b0/layer/softness'] == 'softkrotov']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [plt.cm.Greens(0.5), plt.cm.Blues(1.0)]\n",
    "\n",
    "plt.rcParams.update({'font.size': 20,\n",
    "                    'lines.linewidth':3,\n",
    "                    'lines.linestyle':'-',\n",
    "                    'lines.marker':None,\n",
    "                    'lines.markersize':8})\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "ax = plt.gca()\n",
    "\n",
    "font_size=25\n",
    "\n",
    "for i, d in enumerate(data):\n",
    "    plt.plot(d['t'], d['test_acc_mean'], color=colors[i])\n",
    "\n",
    "lines = ax.get_lines()\n",
    "\n",
    "dummy_lines = []\n",
    "for b in ['-','--']:\n",
    "    dummy_lines.append(ax.plot([],[], c=\"black\", ls = b)[0])\n",
    "for i, d in enumerate(data):\n",
    "    plt.plot(d['t'], d['R1_mean'], color=colors[i], linestyle='--')\n",
    "    \n",
    "scatter = plt.scatter(d.iloc[0]['t']/1.25, d.iloc[0]['test_acc_mean'], s=250, c='black', label='Hard WTA')\n",
    "\n",
    "#legend1 = plt.legend(lines, ['SoftHebb', 'SoftHebb Anti-hebbian'], loc='center left')\n",
    "legend1 = plt.legend([scatter] + lines, ['Hard WTA', 'SoftHebb (without anti-Hebbian)','SoftHebb (with anti-Hebbian)'], bbox_to_anchor=(1, 0.5), loc='lower left', fontsize=font_size)\n",
    "legend2 = plt.legend(dummy_lines, [\"accuracy\", 'R1 features'], loc='lower left', bbox_to_anchor=(1, 0.), fontsize=font_size)\n",
    "#legend3 = plt.legend(handles=[scatter], bbox_to_anchor=(1.4, 0.9))\n",
    "\n",
    "ax.add_artist(legend1)\n",
    "ax.add_artist(legend2)\n",
    "#ax.add_artist(legend3)\n",
    "\n",
    "plt.xticks(np.linspace(0,96,9), fontsize=font_size)\n",
    "plt.title('Softhebb Regimes', fontsize=font_size)\n",
    "plt.xscale('log')\n",
    "ax.set_xlabel('Temperature (i.e. softness)', fontsize=font_size)\n",
    "ax.set_ylabel(\"Accuracy (\\%) and\\n \\# of R1 features\", fontsize=font_size)\n",
    "\n",
    "plt.ylim((-5,105))\n",
    "plt.yticks(np.linspace(0,100,5), fontsize=font_size)\n",
    "plt.minorticks_off()\n",
    "\n",
    "plt.xticks([0.0001,0.01,1,100], fontsize=font_size)\n",
    "#plt.legend(loc='upper right', fontsize=14) \n",
    "plt.show()\n",
    "try:\n",
    "    fig.savefig(op.join(STORE_GRAPH, 'SoftHebb_regimes.svg'), format='svg')\n",
    "    fig.savefig(op.join(STORE_GRAPH, 'SoftHebb_regimes.png'))\n",
    "except:\n",
    "    print('---Save not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa565ddc",
   "metadata": {},
   "source": [
    "## B.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af0103",
   "metadata": {},
   "source": [
    "Run the cells under B.1 but change variable ''exp'' to 'SoftHebb4_regimes_multilayer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1049931",
   "metadata": {},
   "source": [
    "## B.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d33b33f",
   "metadata": {},
   "source": [
    "### B.3.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exps = ['radius_linearDecay', 'radius_radiusDecay']\n",
    "exps = ['InitRadiusLinearDecay', 'InitRadiusNormDecay']\n",
    "features = ['b0/layer/radius','dataset_unsup/seed', 'test_acc']\n",
    "labels=['linear decay', 'norm-dependent adaptation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e7234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = [nb_utils.get_data(exp) for exp in exps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f562c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [nb_utils.extract_data(d, features) for d in exp_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d153fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_bar = [d.groupby(features[:-2]).agg({'test_acc': ['mean', 'std']}).droplevel(0,1).reset_index() for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [blues(0.4), blues(1.0)]\n",
    "plt.close('all')\n",
    "plt.rcParams.update({'font.size': 14,\n",
    "                    'lines.linewidth':2,\n",
    "                    'lines.linestyle':'-',\n",
    "                    'lines.markersize':0})\n",
    "\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "ax = plt.gca()\n",
    "\n",
    "for i, e in enumerate(error_bar):\n",
    "    plt.plot(e['b0/layer/radius'], e['mean'], color=colors[i], label=labels[i])\n",
    "plt.legend()    \n",
    "plt.axvspan(0, 1, facecolor=greens(0.1), alpha=0.7, label = 'No Learning')\n",
    "plt.axvspan(1, 2.2, facecolor=greens(0.4), alpha=0.7, label = 'Partial Learning')\n",
    "plt.axvspan(2.2, 40, facecolor=greens(0.8), alpha=0.7, label = 'Learning')\n",
    "plt.minorticks_off()\n",
    "plt.legend(fontsize=font_size, bbox_to_anchor=(1, 0.5))\n",
    "plt.xlim(1,33)\n",
    "plt.xscale('log', base=2)\n",
    "eps_ticks = np.concatenate((np.logspace(-1, 0, 2, endpoint=False), np.logspace(0 ,1.5, 3)))\n",
    "\n",
    "ax.set_xticks(eps_ticks)\n",
    "eps_ticks_str = ['{:.2f}'.format(x) for x in eps_ticks]\n",
    "ax.set_xticklabels(eps_ticks_str, fontsize=font_size)\n",
    "\n",
    "accuracy_ticks = np.linspace(0., 100, 5)\n",
    "plt.yticks(np.linspace(0,100, 5), fontsize=font_size)\n",
    "                   \n",
    "ax.set_xlabel(r'Initial radius', fontsize=font_size)\n",
    "ax.set_ylabel('Accuracy (\\%)', fontsize=font_size)\n",
    "try:\n",
    "    fig.savefig(op.join(STORE_GRAPH,'Init_radius.svg'), format='svg')\n",
    "    fig.savefig(op.join(STORE_GRAPH,'Init_radius.png'))\n",
    "except:\n",
    "    print('---Save not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c595f3d",
   "metadata": {},
   "source": [
    "### B.3.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['speed_linearDecay', 'speed_radiusDecay']\n",
    "models = ['ConvergenceLinearDecay', 'ConvergenceNormDecay']\n",
    "labels=['linear decay', 'radius dependent']\n",
    "colors = [blues(0.4), blues(1.0)]\n",
    "wts=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = nb_utils.load_data(models, 't1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.rcParams.update({'font.size': 14,\n",
    "                    'lines.linewidth':2,\n",
    "                    'lines.linestyle':'-',\n",
    "                    'lines.markersize':0})\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "\n",
    "ax = plt.gca()\n",
    "for i, data in enumerate(datas):\n",
    "    plt.plot(np.cumsum(data[:,0]), nb_utils.moving_avg(data[:,wts]), \"-\",  label=labels[i], color=colors[i])\n",
    "    \n",
    "plt.xlabel('xlabel')\n",
    "plt.ylabel('ylabel')\n",
    "ax.set_xlabel('Training examples', fontsize=font_size)\n",
    "\n",
    "plt.xticks([0,25000,50000], fontsize=font_size)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=font_size)\n",
    "\n",
    "plt.minorticks_off()\n",
    "if wts==5:\n",
    "    plt.yticks(np.linspace(0,0.1, 9), fontsize=font_size)\n",
    "    plt.ylim((-0,0.1))\n",
    "    #plt.yscale('log', base=10)\n",
    "    ax.set_ylabel('lr')\n",
    "    plt.title('learning rate over time', fontsize=font_size)\n",
    "if wts==4:\n",
    "    plt.yticks(np.linspace(0,100, 5), fontsize=font_size)\n",
    "    plt.ylim((-2,100))\n",
    "    ax.set_ylabel('Number of R1 features', fontsize=font_size)\n",
    "    #plt.title('R1 features', fontsize=16) \n",
    "if wts==3:\n",
    "    plt.yticks(np.linspace(0,5, 5), fontsize=font_size)\n",
    "    plt.yscale('log', base=10)\n",
    "    plt.ylim((-0,5))\n",
    "    ax.set_ylabel('mean distance to a radius of 1', fontsize=font_size)\n",
    "    plt.title('Speed of convergence for 1 epochs', fontsize=font_size)\n",
    "if wts==2:\n",
    "    ax.set_ylabel('Accuracy %', fontsize=font_size)\n",
    "    plt.yticks(np.linspace(20,100, 9), fontsize=font_size)\n",
    "    plt.title('Running accuracy (moving average)', fontsize=font_size)\n",
    "if wts==1:\n",
    "    #plt.ylim((20, 1500))\n",
    "    ax.set_ylabel('Cross entropy Loss', fontsize=font_size)\n",
    "    plt.title('Running loss (moving average)', fontsize=font_size)\n",
    "try:\n",
    "    fig.savefig(op.join(STORE_GRAPH,'speed.png'), format='png')\n",
    "    fig.savefig(op.join(STORE_GRAPH,'speed.svg'), format='svg')\n",
    "except:\n",
    "    print('---Save not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17b326",
   "metadata": {},
   "source": [
    "## B.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in ws06\n",
    "# exp = 'SoftHebb_neurons'\n",
    "# data, folders = get_data(exp, return_folders=True)\n",
    "\n",
    "# meta_exp = 'SoftHebb_neurons'\n",
    "# exps = ['double', 'original', 'half']\n",
    "meta_exp = 'CIFAR10_SoftHebb4'\n",
    "exps = ['_Double', '', '_Half']\n",
    "data, folders = {}, {}\n",
    "for exp in exps:\n",
    "#     data[exp], folders[exp] = nb_utils.get_data(meta_exp + '_' + exp, return_folders=True)\n",
    "    data[exp], folders[exp] = nb_utils.get_data(meta_exp + exp, return_folders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp == 'SoftHebb_neurons':\n",
    "    new_data = add_column_to_folder(data, folders) # add n_neurons column based on experiment code\n",
    "    split_data = split_data_on_column(new_data, value='n_neurons') # split list into lists of dataframes with the same 'n_neurons'\n",
    "else:\n",
    "    split_data = data\n",
    "agg_data = {}\n",
    "for val, i_data in split_data.items():\n",
    "    agg_data[val] = nb_utils.error(i_data, bars=16) # get aggregate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store for other experiments\n",
    "default_SoftHebb_CIFAR10 = {'mean': agg_data['']['test_acc', 'mean'].tolist()[0], \n",
    "                            'std': agg_data['']['test_acc', 'std'].tolist()[0], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "green = cm.get_cmap('Greens')\n",
    "# x = ['1 layer', '2 layers', '3 layers']\n",
    "x = ['48', '96', '192']\n",
    "x_2_exp = {'48': '_Half', '96': '', '192': '_Double'}\n",
    "\n",
    "# not realtest\n",
    "# y = [[agg_data[int(x[0])]['test_acc', 'mean'].tolist()[0],80.31,agg_data[int(x[2])]['test_acc', 'mean'].tolist()[0],]]\n",
    "# error=[[agg_data[int(x[0])]['test_acc', 'std'].tolist()[0],0.148421936,agg_data[int(x[2])]['test_acc', 'std'].tolist()[0]]]\n",
    "\n",
    "y = [[agg_data[x_2_exp[i_x]]['test_acc', 'mean'].tolist()[0] for i_x in x]]\n",
    "error = [[agg_data[x_2_exp[i_x]]['test_acc', 'std'].tolist()[0] for i_x in x]]\n",
    "\n",
    "font_size=25\n",
    "label = ['SoftHebb (ours)']\n",
    "colors = [ plt.cm.Blues(1.0)]\n",
    "plt.rcParams.update({'font.size': 40,\n",
    "                    'lines.linewidth':4,\n",
    "                    'lines.linestyle':'-',\n",
    "                    'lines.markersize':1})\n",
    "fig =  plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)#, aspect=0.08)\n",
    "for i in range(len(label))[::-1]:\n",
    "    plt.errorbar(x, y[i], error[i], color=colors[i], label=label[i])\n",
    "plt.ylabel(\"Accuracy (\\%)\", fontsize=font_size)\n",
    "#plt.xlabel(\"#layer\")\n",
    "plt.xlabel(\"Number of neurons 1st layer\", fontsize=font_size)\n",
    "plt.title(\"CIFAR10 width comparison\", fontsize=font_size)\n",
    "# plt.ylim((55,82))\n",
    "# plt.ylim((70,85)) # overriden by yticks\n",
    "plt.xticks([0, 1, 2],x, fontsize=font_size)\n",
    "# plt.xticks([0, 1, 2],[str(i_x) for i_x in x], fontsize=font_size)\n",
    "plt.minorticks_off()\n",
    "#ax.set_xticks(ax.get_xticks()[1:])\n",
    "#ax.set_xticks([])\n",
    "# plt.yticks(np.linspace(56, 80,4), fontsize=font_size)\n",
    "plt.yticks(np.linspace(70, 85, 4), fontsize=font_size)\n",
    "\n",
    "# plt.legend(fontsize=font_size, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.legend(fontsize=font_size, loc='right', bbox_to_anchor=(1, 0.25))\n",
    "try:\n",
    "    STORE_GRAPH = 'images' # check !pwd\n",
    "    fig.savefig(op.join(STORE_GRAPH, 'width_comparison.png'))\n",
    "    fig.savefig(op.join(STORE_GRAPH, 'width_comparison.svg'), format='svg')\n",
    "except:\n",
    "    print('---Save not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bd775",
   "metadata": {},
   "source": [
    "## B.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "name_model = 'CIFAR10_HardWTA4'\n",
    "# B\n",
    "name_model = 'CIFAR10_SoftHebb4'\n",
    "# C\n",
    "name_model = 'CIFAR10_BP4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sup = 'CIFAR10_50'\n",
    "dataset_sup_config = load_config_dataset(dataset_sup, validation=False)\n",
    "dataset_unsup = 'CIFAR10_1'\n",
    "dataset_unsup_config = load_config_dataset(dataset_unsup, validation=False)\n",
    "label_dict = labels_dict[dataset_sup.split('_')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = nb_utils.find_checkpoint(os.path.join(SEARCH, name_model))\n",
    "assert model_checkpoint is not None, \"Checkpoint of trained model not found\"\n",
    "model = nb_utils.resume_model(name_model, model_path_override=os.path.join(model_checkpoint, 'checkpoint.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68228606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order weight for a better visualization\n",
    "weight = model.blocks[0].layer.weight.detach().cpu()\n",
    "out_channel, in_channel, k1, k2 = weight.shape\n",
    "weight_reshape = weight.reshape((out_channel, in_channel, (k1 * k2)))\n",
    "order = weight_reshape.std(2).mean(1)\n",
    "sorted, indices = torch.sort(order)\n",
    "weight = weight[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ffb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_utils.plot_filter(weight.transpose(1,2).transpose(2,3).numpy(), 15, 8, title=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19839d",
   "metadata": {},
   "source": [
    "## B.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0639b9",
   "metadata": {},
   "source": [
    "Figure 5.A is the same as B.6.A\n",
    "For B.6.A, run the cells for 5.A but use random model (further instructions in the code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b92de9b",
   "metadata": {},
   "source": [
    "## B.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262fbe60",
   "metadata": {},
   "source": [
    "Figure 5.C and 5.D are the same as B.7.A\n",
    "For B.7.B, run the cells for 5.C and 5.D but use random model (further instructions in the code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222eb540",
   "metadata": {},
   "source": [
    "## B.8 , B.9, B.10, B.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c1ef4",
   "metadata": {},
   "source": [
    "Rerun cells for figure 2, but setting 'n_neurons_per_layer = 250'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
