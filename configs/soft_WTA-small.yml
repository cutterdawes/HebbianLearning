model:
  type: GenHebb
  input_dim: 784
  hidden_dim: 100
  output_dim: 10
  learning_rule: soft_WTA
  activation: relu
  n_hebbian_layers: 2
  kwargs:
    beta: 0.1
training:
  unsupervised:
    epochs: 5
    lr: 0.0001
    batch_size: 64
  supervised:
    epochs: 20
    lr: 0.001
    batch_size: 64