model:
  type: GenHebb
  input_dim: 784
  hidden_dim: 100
  output_dim: 10
  learning_rule: soft_WTA
  activation: none
  n_hebbian_layers: 1
  importance_factor: 0
  kwargs:
    temp: 7
    beta: 0.1
training:
  unsupervised:
    epochs: 10
    lr: 0.03
    batch_size: 128
  supervised:
    epochs: 50
    lr: 0.001
    batch_size: 64